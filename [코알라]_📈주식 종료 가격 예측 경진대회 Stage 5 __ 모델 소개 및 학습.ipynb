{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": " [코알라] 주식 종료 가격 예측 경진대회 Stage 5 : 모델 소개 및 학습\n\n안녕하세요. 데이크루 4기 코알라🐨 팀입니다. \n\n저희는 📈주식 종료 가격 예측 경진대회를 주제로 PBL를 수행합니다.\n\n이번 활동을 통해 논리적인 접근 방식으로 모든 문제를 풀어갈 수 있는 데이커가 되는 것을 최종 목표로 하고 있습니다. \n\nStage 5은 PBL을 구성하는 6가지 단계 중 5번째 단계로서 모델 소개 및 학습 내용을 담고있습니다.\n\n다음의 포스팅은 데이크루 4기 활동으로 인하여 작성되었음을 알려드립니다.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# Stage 5.\n# 1. Review\n\nStage 5의 코드를 정상적으로 실행하기 위해서는, 앞서 Stage 4에서 진행했던 코드를 전부 실행한 상태여야 합니다.\n\n뒤에서 사용할 기본 설정을 위해 저희가 추가적인 코드와 함께 한번에 준비했으니, 이 코드창을 실행시키고 들어가주세요!\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import micropip\nawait micropip.install('requests')",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nimport datetime\nimport requests as req\nimport lightgbm as lgb\nfrom sklearn import linear_model\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.feature_selection import SelectKBest, f_regression, f_classif\nfrom sklearn import metrics\nimport os\nimport sys\nimport random\nimport warnings",
      "metadata": {
        "trusted": true
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "warnings.filterwarnings(action='ignore')\nos.chdir('./practice_data')",
      "metadata": {
        "trusted": true
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "## 과거 상장폐지종목 추출 \ndf_out_tt1 = pd.read_csv('코스피상폐현황.csv', encoding='utf-8')\ndf_out_tt2 = pd.read_csv('코스닥상폐현황.csv', encoding='utf-8')\n\ndf_out_tt1['Market'] = 'KOSPI'\ndf_out_tt2['Market'] = 'KOSDAQ'\ndf_out_tt = pd.concat([df_out_tt1, df_out_tt2])\ndf_out_tt.rename(columns = {\"종목코드\":\"code\", \"회사명\":\"code_nm\", \"폐지일자\":\"out_dt\", \"폐지사유\":\"out_desc\", \"Market\":'market'}, inplace = True)\ndf_out_tt['code'] = df_out_tt['code'].astype(str) \ndf_out_tt['code'] = df_out_tt['code'].str.zfill(6) \ndf_out_tt['out_dt'] = pd.to_datetime(df_out_tt['out_dt'])\ndf_out_tt = df_out_tt[[\"code\", \"code_nm\", \"out_dt\", \"out_desc\", \"market\"]]\n\n## 현재 상장종목 전체 추출 : FinanceDataReader\ndf_in_tt = pd.read_csv('종목리스트.csv', encoding='utf-8-sig')\ndf_in_tt.columns = ['code', 'isu_cd', 'code_nm', 'market', 'dept', 'close', 'changecode', 'changes', 'chagesratio', \n                    'open', 'high', 'low', 'volume', 'amount', 'marcap', 'stocks', 'marketId']\ndf_in_tt = df_in_tt[(df_in_tt['market'] !='KONEX') &                \n                    (df_in_tt.code.str.startswith('5') != True) &   \n                    (df_in_tt.code.str.startswith('6') != True) &   \n                    (df_in_tt.code.str.startswith('7') != True) &   \n                    (df_in_tt.code.str.len() == 6)]\ndf_in_tt['in_yn'] = 1  \ndf_in_tt['code'] = df_in_tt['code'].astype(str) \ndf_in_tt['code'] = df_in_tt['code'].str.zfill(6) \ndf_in_tt['close'] = df_in_tt['close'].astype(str)\ndf_in_tt['changecode'] = df_in_tt['close'].astype(str)\n\n## 과거 상장폐지종목 + 현재 상장종목 결합\nend_dt = datetime.datetime.now().strftime('%Y%m%d')   \ndf_stock_code = pd.merge(df_out_tt, df_in_tt, how='outer', on='code', suffixes=('_old', '_new'))\ndf_stock_code['in_yn'] = np.where(df_stock_code['in_yn'].notnull(), df_stock_code['in_yn'], 0)\ndf_stock_code['market'] = np.where(df_stock_code['market_new'].notnull(), df_stock_code['market_new'], df_stock_code['market_old'])\ndf_stock_code['code_nm'] = np.where(df_stock_code['code_nm_new'].notnull(), df_stock_code['code_nm_new'], df_stock_code['code_nm_old'])\ndf_stock_code['base_dt'] = pd.to_datetime(end_dt)\ndf_stock_code.rename(columns = {\"ListingDate\":\"in_dt\"}, inplace = True) \ndf_stock_code = df_stock_code.sort_values(by=['code', 'base_dt'], axis=0, ascending=[True, False])  \ndf_stock_code = df_stock_code.drop_duplicates(['code'], keep='first')\ndf_stock_code = df_stock_code[['code','code_nm','in_yn','market','out_dt','out_desc','base_dt']]\ndf_stock_code = df_stock_code[df_stock_code['code']=='005930']\n\ndf_price = pd.read_csv('주가파일.csv', encoding='utf-8')\ndf_price['dt'] = pd.to_datetime(df_price['dt'])\ndf_price['dt_base'] = pd.to_datetime(df_price['dt_base'])\ndf_price['code'] = df_price['code'].astype(str)\ndf_price['code'] = df_price['code'].str.zfill(6)\n\ndf_usdkrw = pd.read_csv('환율파일.csv', encoding='utf-8')    \ndf_index = pd.read_csv('주가지수.csv', encoding='utf-8')\ndf_index['dt'] = pd.to_datetime(df_index['dt'])\ndf_usdkrw['dt'] = pd.to_datetime(df_usdkrw['dt'])\n\n## 학습 대상 종목 선정 \nstock_list = pd.read_csv(\"./stock_list.csv\")\nstock_list['종목코드'] = stock_list['종목코드'].apply(lambda x : str(x).zfill(6))\nstock_list.rename(columns = {\"종목코드\":\"code\"}, inplace = True) \n\ndf_master = df_price[(df_price['open'] > 0) & (df_price['end'] > 0) & (df_price['cnt'] > 0)]\ndf_master['amt']  = df_master['end'] * df_master['cnt']   \ndf_amt = df_master.groupby(['code'], as_index=False)[['end', 'cnt', 'amt']].mean()\ndf_amt = df_amt[(df_amt['end'] >= 3000) & (df_amt['amt'] >= 2000000000)]     \ndl_amt = df_amt['code'].values.tolist() \ndl_200 = stock_list['code'].values.tolist() \ndl_amt = set(dl_amt + dl_200)\ndf_master = df_master[df_master['code'].isin(dl_amt)]\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "## Target 생성\ndf_master = df_master.sort_values(by=['code', 'dt'], axis=0, ascending=[True, False])  \ndf_master['num'] = df_master.groupby('code')['dt'].rank(ascending=True).astype(int)\ndf_master['y_01d_yn']  = np.where(df_master['code'] == df_master['code'].shift(1), 1, 0)  \ndf_master['y_02d_yn']  = np.where(df_master['code'] == df_master['code'].shift(2), 1, 0)  \ndf_master['y_03d_yn']  = np.where(df_master['code'] == df_master['code'].shift(3), 1, 0)  \ndf_master['y_04d_yn']  = np.where(df_master['code'] == df_master['code'].shift(4), 1, 0)  \ndf_master['y_05d_yn']  = np.where(df_master['code'] == df_master['code'].shift(5), 1, 0)  \ndf_master['y_rt_01d_eeup']  = df_master['end'].shift(1) / df_master['end']\ndf_master['y_rt_02d_eeup']  = df_master['end'].shift(2) / df_master['end']\ndf_master['y_rt_03d_eeup']  = df_master['end'].shift(3) / df_master['end']\ndf_master['y_rt_04d_eeup']  = df_master['end'].shift(4) / df_master['end']\ndf_master['y_rt_05d_eeup']  = df_master['end'].shift(5) / df_master['end']\n\ndf_master = df_master.sort_values(by=['code', 'dt'], axis=0) \ndf_master = pd.merge(df_master, df_index[['dt', 'end_kpi', 'end_ksd']], how='left', on='dt')\ndf_master = pd.merge(df_master, df_stock_code[['code', 'market']], how='left', on='code')\ndf_master['x_rt_end_index']  = np.where(df_master['market'] == 'KOSPI', df_master['end'] / df_master['end_kpi'], \n                                        df_master['end'] / df_master['end_ksd'])\ndf_master = pd.merge(df_master, df_usdkrw[['dt', 'end_usdkrw']], how='left', on='dt')\ndf_master['x_rt_end_usdkrw'] = df_master['end'] / df_master['end_usdkrw']\n\n## 시/저/고/저 파생\ndf_master['avg_amt_20'] = df_master['amt'].rolling(window=20).mean() \ndf_master['x_rt_h_l']  = np.where(df_master['low'] == 0,  np.nan, df_master['high'] / df_master['low'])      \ndf_master['x_rt_e_s']  = np.where(df_master['open'] == 0,  np.nan, df_master['end'] / df_master['open'])   \ndf_master['x_rt_e_l']  = np.where(df_master['low'] == 0,  np.nan, df_master['end'] / df_master['low']) \ndf_master['x_rt_h_s']  = np.where(df_master['open'] == 0,  np.nan, df_master['high'] / df_master['open'])   \ndf_master['x_rt_s_l']  = np.where(df_master['low'] == 0,  np.nan, df_master['open'] / df_master['low'])  \ndf_master['x_rt_bf_1']  = np.where(df_master['end'].shift(1) == 0,   np.nan, df_master['end'] / df_master['end'].shift(1))\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "## 과거 평균 대비 이격도 Feature \nk  = 3; k2 = 10; k3 = 40; k4 = 80\n\ndl_num_cols = ['open', 'high', 'low', 'end', 'amt', 'end_kpi', 'end_ksd', 'x_rt_end_index',\n               'x_rt_bf_1', 'x_rt_e_s', 'x_rt_e_l', 'end_usdkrw', 'x_rt_end_usdkrw' ] \n\ndl_cols =[('x_rt_bf_1_' + w) for w in dl_num_cols]\ndf_master[dl_cols]  = np.where(df_master[dl_num_cols].shift(1) == 0,  np.nan, df_master[dl_num_cols] / df_master[dl_num_cols].shift(1))\n\ndl_cols =[('x_rt_ma_k_' + w) for w in dl_num_cols]\ndf_master[dl_cols]  = np.where(df_master[dl_num_cols].rolling(window=k).mean() == 0,  np.nan, \n                      (df_master[dl_num_cols]-df_master[dl_num_cols].rolling(window=k).mean()) / df_master[dl_num_cols].rolling(window=k2).mean())  \ndf_master[dl_cols] = np.where(df_master[dl_num_cols] >= df_master[dl_num_cols].rolling(window=k).mean(), \n                      abs(df_master[dl_cols]), -abs(df_master[dl_cols]))                                                \n                        \ndl_cols =[('x_rt_ma_k2_' + w) for w in dl_num_cols]\ndf_master[dl_cols]  = np.where(df_master[dl_num_cols].rolling(window=k2).mean() == 0,  np.nan, \n                      (df_master[dl_num_cols]-df_master[dl_num_cols].rolling(window=k2).mean()) / df_master[dl_num_cols].rolling(window=k2).mean())  \ndf_master[dl_cols] = np.where(df_master[dl_num_cols] >= df_master[dl_num_cols].rolling(window=k2).mean(), \n                      abs(df_master[dl_cols]), -abs(df_master[dl_cols]))\n                        \ndl_cols =[('x_rt_ma_k_k2_' + w) for w in dl_num_cols]\ndf_master[dl_cols]  = np.where(df_master[dl_num_cols].rolling(window=k2).mean() == 0,  np.nan, \n                      (df_master[dl_num_cols].rolling(window=k).mean()-df_master[dl_num_cols].rolling(window=k2).mean()) / df_master[dl_num_cols].rolling(window=k2).mean())  \ndf_master[dl_cols] = np.where(df_master[dl_num_cols].rolling(window=k).mean() >= df_master[dl_num_cols].rolling(window=k2).mean(), \n                      abs(df_master[dl_cols]), -abs(df_master[dl_cols]))\n\ndl_cols =[('x_rt_ma_k3_' + w) for w in dl_num_cols]\ndf_master[dl_cols]  = np.where(df_master[dl_num_cols].rolling(window=k3).mean() == 0,  np.nan, \n\t\t\t\t\t\t\t (df_master[dl_num_cols]-df_master[dl_num_cols].rolling(window=k3).mean()) / df_master[dl_num_cols].rolling(window=k2).mean())  \ndf_master[dl_cols] = np.where(df_master[dl_num_cols] >= df_master[dl_num_cols].rolling(window=k3).mean(), \n\t\t\t\t\t\t\t abs(df_master[dl_cols]), -abs(df_master[dl_cols]))\n\t\t\t\t\t\t\t \ndl_cols =[('x_rt_ma_k_k3_' + w) for w in dl_num_cols]\ndf_master[dl_cols]  = np.where(df_master[dl_num_cols].rolling(window=k3).mean() == 0,  np.nan, \n\t\t\t\t\t\t\t (df_master[dl_num_cols].rolling(window=k).mean()-df_master[dl_num_cols].rolling(window=k3).mean()) / df_master[dl_num_cols].rolling(window=k2).mean())  \ndf_master[dl_cols] = np.where(df_master[dl_num_cols].rolling(window=k).mean() >= df_master[dl_num_cols].rolling(window=k3).mean(), \n\t\t\t\t\t\t\t abs(df_master[dl_cols]), -abs(df_master[dl_cols]))\n\t\t\t\t\t\t\t \ndl_cols =[('x_rt_ma_k2_k3_' + w) for w in dl_num_cols]\ndf_master[dl_cols]  = np.where(df_master[dl_num_cols].rolling(window=k3).mean() == 0,  np.nan, \n\t\t\t\t\t\t\t (df_master[dl_num_cols].rolling(window=k2).mean()-df_master[dl_num_cols].rolling(window=k3).mean()) / df_master[dl_num_cols].rolling(window=k2).mean())  \ndf_master[dl_cols] = np.where(df_master[dl_num_cols].rolling(window=k2).mean() >= df_master[dl_num_cols].rolling(window=k3).mean(), \n\t\t\t\t\t\t\t abs(df_master[dl_cols]), -abs(df_master[dl_cols]))\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t \ndl_cols =[('x_rt_ma_k4_' + w) for w in dl_num_cols]\ndf_master[dl_cols]  = np.where(df_master[dl_num_cols].rolling(window=k4).mean() == 0,  np.nan, \n\t\t\t\t\t\t\t (df_master[dl_num_cols]-df_master[dl_num_cols].rolling(window=k4).mean()) / df_master[dl_num_cols].rolling(window=k2).mean())  \ndf_master[dl_cols] = np.where(df_master[dl_num_cols] >= df_master[dl_num_cols].rolling(window=k4).mean(), \n\t\t\t\t\t\t\t abs(df_master[dl_cols]), -abs(df_master[dl_cols]))\n\t\t\t\t\t\t\t \ndl_cols =[('x_rt_ma_k_k4_' + w) for w in dl_num_cols]\ndf_master[dl_cols]  = np.where(df_master[dl_num_cols].rolling(window=k4).mean() == 0,  np.nan, \n\t\t\t\t\t\t\t (df_master[dl_num_cols].rolling(window=k).mean()-df_master[dl_num_cols].rolling(window=k4).mean()) / df_master[dl_num_cols].rolling(window=k2).mean())  \ndf_master[dl_cols] = np.where(df_master[dl_num_cols].rolling(window=k).mean() >= df_master[dl_num_cols].rolling(window=k4).mean(), \n\t\t\t\t\t\t\t abs(df_master[dl_cols]), -abs(df_master[dl_cols]))\n\t\t\t\t\t\t\t \ndl_cols =[('x_rt_ma_k2_k4_' + w) for w in dl_num_cols]\ndf_master[dl_cols]  = np.where(df_master[dl_num_cols].rolling(window=k4).mean() == 0,  np.nan, \n\t\t\t\t\t\t\t (df_master[dl_num_cols].rolling(window=k2).mean()-df_master[dl_num_cols].rolling(window=k4).mean()) / df_master[dl_num_cols].rolling(window=k2).mean())  \ndf_master[dl_cols] = np.where(df_master[dl_num_cols].rolling(window=k2).mean() >= df_master[dl_num_cols].rolling(window=k4).mean(), \n\t\t\t\t\t\t\t abs(df_master[dl_cols]), -abs(df_master[dl_cols]))\n\t\t\t\t\t\t\t \ndl_cols =[('x_rt_ma_k3_k4_' + w) for w in dl_num_cols]\ndf_master[dl_cols]  = np.where(df_master[dl_num_cols].rolling(window=k4).mean() == 0,  np.nan, \n\t\t\t\t\t\t\t (df_master[dl_num_cols].rolling(window=k3).mean()-df_master[dl_num_cols].rolling(window=k4).mean()) / df_master[dl_num_cols].rolling(window=k2).mean())  \ndf_master[dl_cols] = np.where(df_master[dl_num_cols].rolling(window=k3).mean() >= df_master[dl_num_cols].rolling(window=k4).mean(), \n\t\t\t\t\t\t\t abs(df_master[dl_cols]), -abs(df_master[dl_cols]))\n\n## 단순이동평균 대비 지수이동평균 대비 이격도\nfor col in dl_num_cols:\n\tstr_ma05 = 'ma05_' + col\n\tstr_ema05 = 'ema05_' + col\n\tstr_rt_ema05_ma05 = 'x_rt_ema05_ma05_' + col\n\tdf_master[str_ma05] = df_master[col].rolling(window=5).mean() \t\t\n\tdf_master[str_ema05]  = df_master[col].shift(5) \n\ttmp1 = 0.7 \t\n\tfor i in range(5):\n\t\tdf_master[str_ema05]  = (df_master[col].shift(5-1-i) * tmp1) + (df_master[str_ema05] * (1-tmp1))\t\t\n\tdf_master[str_rt_ema05_ma05] = np.where(df_master[str_ma05] == 0,  np.nan, (df_master[str_ema05]-df_master[str_ma05]) / df_master[str_ma05])  \n\tdf_master[str_rt_ema05_ma05] = np.where(df_master[str_ema05] >= df_master[str_ma05], abs(df_master[str_rt_ema05_ma05]), -abs(df_master[str_rt_ema05_ma05]))\t\t   \n\n\tstr_ma20 = 'ma20_' + col\n\tstr_ema20 = 'ema20_' + col\n\tstr_rt_ema20_ma20 = 'x_rt_ema20_ma20_' + col\n\tdf_master[str_ma20] = df_master[col].rolling(window=20).mean() \t\t\n\tdf_master[str_ema20]  = df_master[col].shift(20) \n\ttmp1 = 0.7 \t\n\tfor i in range(20):\n\t\tdf_master[str_ema20]  = (df_master[col].shift(20-1-i) * tmp1) + (df_master[str_ema20] * (1-tmp1))\t\t\n\tdf_master[str_rt_ema20_ma20] = np.where(df_master[str_ma20] == 0,  np.nan, (df_master[str_ema20]-df_master[str_ma20]) / df_master[str_ma20])  \n\tdf_master[str_rt_ema20_ma20] = np.where(df_master[str_ema20] >= df_master[str_ma20], abs(df_master[str_rt_ema20_ma20]), -abs(df_master[str_rt_ema20_ma20]))\t\t   \n\n\tstr_ma40 = 'ma40_' + col\n\tstr_ema40 = 'ema40_' + col\n\tstr_rt_ema40_ma40 = 'x_rt_ema40_ma40_' + col\n\tdf_master[str_ma40] = df_master[col].rolling(window=40).mean() \t\t\n\tdf_master[str_ema40]  = df_master[col].shift(40) \n\ttmp1 = 0.7 \t\n\tfor i in range(40):\n\t\tdf_master[str_ema40]  = (df_master[col].shift(40-1-i) * tmp1) + (df_master[str_ema40] * (1-tmp1))\t\t\n\tdf_master[str_rt_ema40_ma40] = np.where(df_master[str_ma40] == 0,  np.nan, (df_master[str_ema40]-df_master[str_ma40]) / df_master[str_ma40])  \n\tdf_master[str_rt_ema40_ma40] = np.where(df_master[str_ema40] >= df_master[str_ma40], abs(df_master[str_rt_ema40_ma40]), -abs(df_master[str_rt_ema40_ma40]))\t\t   \n\n\tstr_ma80 = 'ma80_' + col\n\tstr_ema80 = 'ema80_' + col\n\tstr_rt_ema80_ma80 = 'x_rt_ema80_ma80_' + col\n\tdf_master[str_ma80] = df_master[col].rolling(window=80).mean() \t\t\n\tdf_master[str_ema80]  = df_master[col].shift(80) \n\ttmp1 = 0.7 \t\n\tfor i in range(80):\n\t\tdf_master[str_ema80]  = (df_master[col].shift(80-1-i) * tmp1) + (df_master[str_ema80] * (1-tmp1))\t\t\n\tdf_master[str_rt_ema80_ma80] = np.where(df_master[str_ma80] == 0,  np.nan, (df_master[str_ema80]-df_master[str_ma80]) / df_master[str_ma80])  \n\tdf_master[str_rt_ema80_ma80] = np.where(df_master[str_ema80] >= df_master[str_ma80], abs(df_master[str_rt_ema80_ma80]), -abs(df_master[str_rt_ema80_ma80]))\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "## Lag 변수\t\t\ndl_code = ['x_rt_bf_1', 'x_rt_h_l', 'x_rt_e_s', 'x_rt_e_l']\nfor m in range(10):   \n\tfor code in dl_code:\n\t\tstr_x = 'x_lag_' + code + str(m+1) \t\n\t\tdf_master[str_x] = df_master[code].shift(m+1)\n\t\n\n## NVI(Negative Volume Index)\n\n# end + cnt\t\ndf_master['x_ind_nvi_10'] = 100\ndf_master['cnt_tmp'] = df_master['cnt'].shift(10)\ndf_master['end_tmp'] = df_master['end'].shift(10) \nfor i in range(9, -1, -1):  \n\tdf_master['tmp1'] = np.where(df_master['end_tmp'] == 0, np.nan, \n\t\t\t\t\t\t\t\t   ((df_master['end'].shift(i) - df_master['end_tmp']) / df_master['end_tmp']) * 100)\n\tdf_master['x_ind_nvi_10'] = np.where(df_master['cnt'].shift(i) < df_master['cnt_tmp'], df_master['x_ind_nvi_10'] + df_master['tmp1'], \n\t\t\t\t\t\t\t\t\t\tdf_master['x_ind_nvi_10'])\n\tdf_master['cnt_tmp'] = df_master['cnt'].shift(i)\n\tdf_master['end_tmp'] = df_master['end'].shift(i)\n\t\n# high + cnt\t\ndf_master['x_ind_h_nvi_10'] = 100\ndf_master['cnt_tmp'] = df_master['cnt'].shift(10)\ndf_master['high_tmp'] = df_master['high'].shift(10) \nfor i in range(9, -1, -1):  \n\tdf_master['tmp1'] = np.where(df_master['high_tmp'] == 0, np.nan, \n\t\t\t\t\t\t\t\t   ((df_master['high'].shift(i) - df_master['high_tmp']) / df_master['high_tmp']) * 100)\n\tdf_master['x_ind_h_nvi_10'] = np.where(df_master['cnt'].shift(i) < df_master['cnt_tmp'], df_master['x_ind_h_nvi_10'] + df_master['tmp1'], \n\t\t\t\t\t\t\t\t\t\tdf_master['x_ind_h_nvi_10'])\n\tdf_master['cnt_tmp'] = df_master['cnt'].shift(i)\n\tdf_master['high_tmp'] = df_master['high'].shift(i)\n\t\n\n# low + cnt\t\ndf_master['x_ind_l_nvi_10'] = 100\ndf_master['cnt_tmp'] = df_master['cnt'].shift(10)\ndf_master['low_tmp'] = df_master['low'].shift(10) \nfor i in range(9, -1, -1):  \n\tdf_master['tmp1'] = np.where(df_master['low_tmp'] == 0, np.nan, \n\t\t\t\t\t\t\t\t   ((df_master['low'].shift(i) - df_master['low_tmp']) / df_master['low_tmp']) * 100)\n\tdf_master['x_ind_l_nvi_10'] = np.where(df_master['cnt'].shift(i) < df_master['cnt_tmp'], df_master['x_ind_l_nvi_10'] + df_master['tmp1'], \n\t\t\t\t\t\t\t\t\t\tdf_master['x_ind_l_nvi_10'])\n\tdf_master['cnt_tmp'] = df_master['cnt'].shift(i)\n\tdf_master['low_tmp'] = df_master['low'].shift(i)\n\t\t\n\t\n## PVI(Positive Volume Index)\n\n# end + cnt\t\t\t\t\t\t\t\t\t\t\ndf_master['x_ind_pvi_10'] = 100\ndf_master['cnt_tmp'] = df_master['cnt'].shift(10) \ndf_master['end_tmp'] = df_master['end'].shift(10) \nfor i in range(9, -1, -1):  \n\tdf_master['tmp1'] = np.where(df_master['end_tmp'] == 0, np.nan, \n\t\t\t\t\t\t\t\t   ((df_master['end'].shift(i) - df_master['end_tmp']) / df_master['end_tmp']) * 100)\n\tdf_master['x_ind_pvi_10'] = np.where(df_master['cnt'].shift(i) > df_master['cnt_tmp'], df_master['x_ind_pvi_10'] + df_master['tmp1'], \n\t\t\t\t\t\t\t\t\t\tdf_master['x_ind_pvi_10'])\n\tdf_master['cnt_tmp'] = df_master['cnt'].shift(i)\n\tdf_master['end_tmp'] = df_master['end'].shift(i)\n\t\n# high + cnt\t\t\t\t\t\t\t\t\t\t\ndf_master['x_ind_h_pvi_10'] = 100\ndf_master['cnt_tmp'] = df_master['cnt'].shift(10) \ndf_master['high_tmp'] = df_master['high'].shift(10) \nfor i in range(9, -1, -1):  \n\tdf_master['tmp1'] = np.where(df_master['high_tmp'] == 0, np.nan, \n\t\t\t\t\t\t\t\t   ((df_master['high'].shift(i) - df_master['high_tmp']) / df_master['high_tmp']) * 100)\n\tdf_master['x_ind_h_pvi_10'] = np.where(df_master['cnt'].shift(i) > df_master['cnt_tmp'], df_master['x_ind_h_pvi_10'] + df_master['tmp1'], \n\t\t\t\t\t\t\t\t\t\tdf_master['x_ind_h_pvi_10'])\n\tdf_master['cnt_tmp'] = df_master['cnt'].shift(i)\n\tdf_master['high_tmp'] = df_master['high'].shift(i)\n\t\n# low + cnt\t\t\t\t\t\t\t\t\t\t\ndf_master['x_ind_l_pvi_10'] = 100\ndf_master['cnt_tmp'] = df_master['cnt'].shift(10) \ndf_master['low_tmp'] = df_master['low'].shift(10) \nfor i in range(9, -1, -1):  \n\tdf_master['tmp1'] = np.where(df_master['low_tmp'] == 0, np.nan, \n\t\t\t\t\t\t\t\t   ((df_master['low'].shift(i) - df_master['low_tmp']) / df_master['low_tmp']) * 100)\n\tdf_master['x_ind_l_pvi_10'] = np.where(df_master['cnt'].shift(i) > df_master['cnt_tmp'], df_master['x_ind_l_pvi_10'] + df_master['tmp1'], \n\t\t\t\t\t\t\t\t\t\tdf_master['x_ind_l_pvi_10'])\n\tdf_master['cnt_tmp'] = df_master['cnt'].shift(i)\n\tdf_master['low_tmp'] = df_master['low'].shift(i)\n\n## Rule Feature\ndf_master['x_cls_01'] = np.where((df_master['open'] < df_master['end']) & (df_master['high'] > df_master['end']) & (df_master['high']/df_master['end'].shift(1) <=1.01), 1,\n\t\t\t\t\t    np.where((df_master['open'] < df_master['end']) & (df_master['high'] > df_master['end']), 2, 3))\n\t\t\t\t\t\t\ndl_list = df_master.columns[pd.Series(df_master.columns).str.startswith('x_cls_')]   \ndf_master[dl_list] = df_master[dl_list].astype(str) \n",
      "metadata": {
        "trusted": true
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "# 2. 모델 소개\n\n## LightGBM\nLightGBM(이하 LGBM)은 트리 기반의 학습 알고리즘인 gradient boosting 방식의 프레임 워크입니다.\n\nLGBM은 유사한 알고리즘인 XGBoost에 비해 훈련 시간이 짧고 성능도 좋아 부스팅 알고리즘에서 가장 많은 주목을 받고 있습니다. \n\nLGBM의 경우 복잡한 것은 파라미터 튜닝입니다. \n\nLGBM은 100개 이상의 파라미터를 가지고 있습니다. 그렇기 때문에 LGBM의 가장 베이직한 파라미터를 아는 것이 구현 시 매우 중요합니다. \n\n\n## LGBM의 대표적인 파라미터\n\n- **n_leaves** : 전체 Tree의 leave 수 이고, 디폴트값은 31입니다.\n\n- **n_estimators** : 생성할 weak learner의 수이고, 너무 크면 과대적합이 발생합니다.\n\n- **max_depth** :  Tree의 최대 깊이를 말합니다. 이 파라미터는 모델 과적합을 다룰 때 사용됩니다. 만약 모델이 과적합된 것 같다고 느끼신다면 max_depth 값을 줄이는 것을 추천드립니다.\n\n- **learning_rate** : 최종 결과에 대한 각각의 Tree에 영향을 미치는 변수입니다. GBM은 초기의 추정값에서 시작하여 각각의Tree 결과를 사용하여 추정값을 업데이트 합니다. 학습 파라미터는 이러한 추정에서 발생하는 변화의 크기를 컨트롤합니다. 일반적인 값은 0.1, 0.001, 0.003 등등이 있습니다.\n\n- **subsample** : weak learner가 학습에 사용하는 데이터 샘플링 비율입니다. 보통 0.5~ 1이 사용되며 값이 낮을수록 과적합이 방지됩니다.\n\n- **colsample_bytree** : 컬럼을 랜덤으로 추출하여 사용해 각각의 다양성을 높입니다. 랜덤포레스트에 있는 기능이며, 보통 정확도를 높이는 효과가 있습니다.\n\n- **reg_alpha** : 가중치에 대한 L1 Regularization 적용 값입니다. 피처 개수가 많을 때 적용을 검토하며 이 값이 클 수록 과적합 감소 효과가 있습니다. 기본값은 0입니다.\n\n- **reg_lambda** : 가중치에 대한 L2 Regularization 적용 값입니다. 피처 개수가 많을 때 적용을 검토하며 이 값이 클 수록 과적합 감소 효과가 있습니다. 기본값은 0입니다.\n\n- **objective** : 목적(손실)함수로 이 함수의 결과값이 최소화되는 방향으로 학습한다. 회귀인지, 분류인지 등에 따라 옵션을 다르게 선택해야 한다. regression, binary, multicalss 모두 가능합니다.\n\n- **min_data_in_leaf** : Leaf가 가지고 있는 최소한의 레코드 수입니다. 디폴트값은 20으로 최적 값입니다. 과적합을 해결할 때 사용되는 파라미터입니다.\n\n- **random_state** :  랜덤 시드 고정 값으로서, 반드시 고정해두고 튜닝해야 합니다.\n\n지금 보시기엔 너무 어려운 내용이 많죠?\n\n지금은 어려울 수도 있습니다. 머신러닝과 관련된 개념이 나오기 때문인데, 나중에 데이커 여러분들이 머신러닝 기초와 의사결정나무(Decision Tree), 랜덤포레스트(Random Forest) 등을 공부하시게 된다면 무슨 소리인지 이해하실 수 있을겁니다.\n\n# 3. 모델링\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "## 분석 모집단 생성\n# public, private에 필요한 데이터 날짜\npub_base_dt = '20211029'  \npri_base_dt = '20211126'  \n## Public, Private 평가 기간\ndl_predict_pub_day = ['2021-11-01', '2021-11-02', '2021-11-03', '2021-11-04', '2021-11-05']   \ndl_predict_pri_day = ['2021-11-29', '2021-11-30', '2021-12-01', '2021-12-02', '2021-12-03']   \n\n## 분석 모집단 생성\ndf_mst = df_master.query(\"num >=101 & cnt > 0 \")\t\n# num은 여기서 인덱스를 의미하는데, 주가를 예측하기 위해선 하루의 데이터가 아니라, 수십일 전까지의 데이터도 필요하기 때문에 앞의 100개의 데이터는 배제.\ndl_col_1 = ['dt', 'code', 'amt']     #날짜, 종목코드, 거래대금\ndl_col_2 = df_mst.columns[pd.Series(df_mst.columns).str.startswith('y_')].values.tolist()   \n#y_로 시작하는 (향후 5일치 주가 향방)값을 리스트로 불러오기.\ndl_x1 = df_mst.columns[pd.Series(df_mst.columns).str.startswith('x_')].values.tolist()\n#x_로 시작하는 값(환율과 주가지수)을 리스트로 불러온다.  \ndl_x2 = ['end', 'avg_amt_20', 'end_usdkrw']  #종가, 거래대금 20일 이동평균, 종가\ndl_x = dl_x1 + dl_x2 \ndf_mst = df_mst[dl_col_1 + dl_col_2 + dl_x]  # 분석에 필요한 컬럼만 추출해 df_mst에 저장.\n\n\n## 학습 조건 설정\ndl_look_after = [1, 2, 3, 4, 5]                        # 예측기간\ndl_n_trin = [800, 400, 200, 100, 50]     # 학습 데이타건수\ndl_target = ['y_rt_01d_eeup', 'y_rt_02d_eeup', 'y_rt_03d_eeup', 'y_rt_04d_eeup', 'y_rt_05d_eeup']   # Target Name\ndl_yn     = ['y_01d_yn', 'y_02d_yn', 'y_03d_yn', 'y_04d_yn', 'y_05d_yn']                            # Target 불능 여부\n\ndf_m2_pre_c1_tot = pd.DataFrame()\ndf_m2_pre_c2_tot = pd.DataFrame()\n\n## 예측기간별 모델링\nfor look in dl_look_after:\n\n\tprint(\"\\n ===> look_after : \",  look)\n\t\n\tdf_mst_tt = df_mst.copy()\n\n\tn_trin_sample = dl_n_trin[look-1]   \n\trandom.seed(123)\n\t\n\tstr_target            = dl_target[look-1]      \n\tstr_yn                = dl_yn[look-1]     \n\tlook_after            = look      \n\tdf_mst_tt.rename(columns = {str_target:'Target'}, inplace = True)    \n\t\n\t## 학습 및 예측 조건\t\n    # c1은 public을 예측하기 위한 데이터, c2는 private을 예측하기 위한 데이터\n    # m1은 훈련용 데이터, m2는 예측용 데이터\n\tdf_m1_tt_c1 = df_mst_tt[(df_mst_tt['dt'] >= '20120101') & (df_mst_tt['dt'] < pub_base_dt)]   \t           \n\tdf_m1_tt_c2 = df_mst_tt[(df_mst_tt['dt'] >= '20120101') & (df_mst_tt['dt'] < pri_base_dt)]   \n\tdf_m2_tt_c1 = df_mst_tt[(df_mst_tt['dt'] == pub_base_dt)]  \n\tdf_m2_tt_c2 = df_mst_tt[(df_mst_tt['dt'] == pri_base_dt)]  \n\tdf_m1_tt_c1 = df_m1_tt_c1[df_m1_tt_c1[str_yn]==1]   \n\tdf_m1_tt_c2 = df_m1_tt_c2[df_m1_tt_c2[str_yn]==1]   \t\n\tdf_m2_tt_c1 = df_m2_tt_c1[df_m2_tt_c1['code'].isin(stock_list['code'])] \n\tdf_m2_tt_c2 = df_m2_tt_c2[df_m2_tt_c2['code'].isin(stock_list['code'])] \n\t\n\t## Target Scaling\t\n    # 이상치(너무 극단적인 데이터)를 제거하기 위해, 상위 20퍼에서 상위 80퍼 안에 들어가는 값만 추출\n\ttarget_out1 = df_m1_tt_c1['Target'].describe(percentiles = [0.2, 0.8])\n\ttarget_out2 = df_m1_tt_c2['Target'].describe(percentiles = [0.2, 0.8])\n\tdf_m1_tt_c1 = df_m1_tt_c1[(df_m1_tt_c1['Target'] >= target_out1['20%']) & (df_m1_tt_c1['Target'] < target_out1['80%'])]\n\tdf_m1_tt_c2 = df_m1_tt_c2[(df_m1_tt_c2['Target'] >= target_out2['20%']) & (df_m1_tt_c2['Target'] < target_out2['80%'])]\n\t\n\n\tdef f_featuring(df_m1_tt, df_m2_tt):\n\t\t\n\t\trandom.seed(123)   \n\t\t\n\t\t## Sampling\t\t\n        # 학습 데이터를 랜덤하게 추출하여 사용\n\t\trows = random.sample(df_m1_tt.index.tolist(), n_trin_sample)     \n\t\tdf_m1_sam = df_m1_tt.loc[rows]\t\t\t\n\t\tdf_m1_sam = df_m1_sam.sort_values(by=['dt'], axis=0)    \t\t\n\t\tdf_m2_sam = df_m2_tt.sort_values(by=['dt'], axis=0)   \n\t\t\n\t\t## X, Y 분리\t\t\t\n\t\tdl_y = df_m1_sam.columns[pd.Series(df_m1_sam.columns).str.startswith('Target')].values.tolist()  \t\t\t\n\t\tdf_m1_x = df_m1_sam[dl_x]\t\n\t\tdf_m2_x = df_m2_sam[dl_x]\t\n\t\tdf_m1_y = df_m1_sam[dl_y]\t\n\t\tdf_m2_y = df_m2_sam[dl_y]\n\t\t\t\n\t\t## Missing 처리\n\t\tdf_m1_x = df_m1_x.replace([np.inf, -np.inf], np.nan)\t\n\t\tdf_m2_x = df_m2_x.replace([np.inf, -np.inf], np.nan)\n        # 뒤에서 로그변환을 해주어야 하는데, log(inf) = 0 이므로, 0이 되지 않게 변환 후 제거\n        \n\t\tdf_m1_x.dropna(how='any', axis=1, inplace=True)  \n        # how='all'은 한 행이 가지는 모든 컬럼이 결측치여야 drop\n\t\tdl_tot_cols = df_m1_x.columns                    \n\t\tdf_m2_x = df_m2_x[dl_tot_cols]                   \n\t\t\t\n\t\tdl_num_cols = df_m1_x._get_numeric_data().columns                    \n\t\tif len(dl_num_cols) >= 1:\n\t\t\timp = SimpleImputer(missing_values=np.nan, strategy='median')\n            # SimpleImputer는 결측치를 대체하는 기법 중 하나로, 숫자형(연속형) 데이터는 strategy = 'median'으로 지정하여 중앙값으로 대체\n            \n\t\t\timp = imp.fit(df_m1_x[dl_num_cols])\n\t\t\tdf_m1_x[dl_num_cols] = imp.transform(df_m1_x[dl_num_cols])\n\t\t\tdf_m2_x[dl_num_cols] = imp.transform(df_m2_x[dl_num_cols])\n\t\t\t\n\t\tdl_cls_cols = df_m1_x.select_dtypes(include=['object']).columns.tolist()\t\t\n\t\tif len(dl_cls_cols) >= 1:\t\t\n            # 문자형 데이터는 결측치를 'missing'이라는 문자로 대체\n\t\t\tdf_m1_x[dl_cls_cols] = df_m1_x[dl_cls_cols].fillna('missing')\n\t\t\tdf_m2_x[dl_cls_cols] = df_m2_x[dl_cls_cols].fillna('missing')\n\n\t\t## Unique 변수 제외 -> 한 변수의 값이 하나밖에 없는 변수를 제거\n\t\tdf_col_list = df_m1_x.apply(pd.Series.nunique) != 1    \n\t\tdf_m1_x = df_m1_x.loc[:, df_col_list]\n\t\tdf_m2_x = df_m2_x.loc[:, df_col_list]\t\t\t\t\n\n\t\t## 연속형 변수 Log 변환 \t\t\n        # 로그변환은 큰 수를 작게 만들거나, 복잡한 계산을 간편하게 할 경우 사용, 왜도와 첨도를 줄여서 데이터 분석 시 의미있는 결과를 도출하기 위함.\n\t\tdl_num_cols = df_m1_x._get_numeric_data().columns   \n\t\tfor col in dl_num_cols:\t\t  \n\t\t\tdf_m1_x[col] = np.where(df_m1_x[col] < 0, -np.log(1 + -df_m1_x[col]), np.log(1 + df_m1_x[col]))   \n\t\t\tdf_m2_x[col] = np.where(df_m2_x[col] < 0, -np.log(1 + -df_m2_x[col]), np.log(1 + df_m2_x[col]))   \n\t\t\t\t\t\n\t\t## 변수 표준화 \n        # 연속형 데이터는 범주형 데이터와 달리 정규화 과정이 필요. 대부분의 알고리즘들이 변수의 범위에 영향을 받기 때문에 각 변수가 가지는 값들의 숫자 범위가 다를 경우, 이를 일정한 범위로 맞춰줘야함.\n\t\tdl_num_cols = df_m1_x._get_numeric_data().columns   \n\t\tscaler = StandardScaler().fit(df_m1_x[dl_num_cols])           \n\t\tdf_m1_x[dl_num_cols] = scaler.transform(df_m1_x[dl_num_cols])  \n\t\tdf_m2_x[dl_num_cols] = scaler.transform(df_m2_x[dl_num_cols])  \n\t\t\n\t\t## One Hot Encoding \t\t\n        # 원-핫 인코딩은 단어 집합의 크기를 벡터의 차원으로 하고, 표현하고 싶은 값의 인덱스에 1의 값을 부여하고, 다른 인덱스에는 0을 부여하는 데이터터의 벡터 표현 방식. \n\t\tdl_cls_cols = df_m1_x.select_dtypes(include=['object']).columns.tolist()\t\t\n\t\tdf_m1_x_dumy = pd.get_dummies(df_m1_x[dl_cls_cols], prefix=dl_cls_cols)   \n\t\tdf_m2_x_dumy = pd.get_dummies(df_m2_x[dl_cls_cols], prefix=dl_cls_cols)\n\t\tdl_col_m1 = df_m1_x_dumy.columns\t\n\t\tdl_col_m2 = df_m2_x_dumy.columns\n\n\t\tdl_col_m1_m2 = list(set(dl_col_m1) - set(dl_col_m2))  \n\t\tif len(dl_col_m1_m2) >= 1:\n\t\t\tfor col in dl_col_m1_m2:\n\t\t\t\tdf_m2_x_dumy[col] = 0\t\t\t\t\t\n\n\t\tdf_m1_x = pd.concat([df_m1_x[dl_num_cols], df_m1_x_dumy], axis=1)\n\t\tdf_m2_x = pd.concat([df_m2_x[dl_num_cols], df_m2_x_dumy], axis=1)\t\n\t\tdf_m1_x, df_m2_x = df_m1_x.align(df_m2_x, join='left', axis=1, fill_value=0)\n\n\t\t## 상관분석\n        # 상관계수가 매우 높은(절댓값이 0.95보다 큰) 컬럼을 제거.\n\t\tcorr_matrix = df_m1_x.corr()\t\t\n\t\tdf_corr_upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))  \n\t\tto_drop = [column for column in df_corr_upper.columns if (any(df_corr_upper[column] >= 0.95) | any(df_corr_upper[column] < -0.95))]   \n\t\tdf_m1_x.drop(to_drop, axis=1, inplace=True)\n\t\tdf_m2_x.drop(to_drop, axis=1, inplace=True)\n\t\t\n\t\t## Feature 안정성 분석\t\t\n\t\tif len(df_m1_x.columns) > 120:   \n\t\t\tnum = 120                    \n\t\telse:\n\t\t\tnum = len(df_m1_x.columns)   \n\t\tmodel = SelectKBest(f_regression, k= num) \n# SelectKBest는 데이터에 컬럼이 너무 많은 경우 모델의 설명력을 가장 높일 수 있는 K개의 컬럼만 선택하게 하는 모듈.\n# 데이터에 컬럼이 너무 많을 경우 모델 학습시간이 오래걸리기도 하고, 과대적합이라는 문제가 발생.        \n# f_regression은 회귀분석의 F-test을 진행하는 모듈. SelectKBest에서 최적의 변수 조합을 찾아내기 위해 사용.\n\t\tcol_list = pd.DataFrame(df_m1_x.columns, columns=['col'])\n\t\tint_num = int(len(df_m1_x)/3)\n\t\tfit = model.fit(df_m1_x[:int_num], df_m1_y[:int_num])\n\t\tscore_list_1 = pd.DataFrame(model.scores_, columns=['score'])\n\t\tfit = model.fit(df_m1_x[int_num:int_num*2], df_m1_y  [int_num:int_num*2])\n\t\tscore_list_2 = pd.DataFrame(model.scores_, columns=['score'])\n\t\tfit = model.fit(df_m1_x[int_num*2:], df_m1_y[int_num*2:])\n\t\tscore_list_3 = pd.DataFrame(model.scores_, columns=['score'])\t\n\t\tdf_importance = pd.concat([col_list, score_list_1, score_list_2, score_list_3], axis=1)     \n\t\tdf_importance['mean'] = df_importance.iloc[:, [1, 2, 3]].min(1)\n\t\tdf_importance['rank'] = df_importance['mean'].rank(ascending=0)  \n\t\tdf_m1_x = df_m1_x[df_importance.loc[df_importance['rank'] <= num, 'col']]\n\t\tdf_m2_x = df_m2_x[df_importance.loc[df_importance['rank'] <= num, 'col']]\t\t\n\t\t\t\t\t\t\n\t\treturn df_m1_x, df_m2_x, df_m1_y, df_m2_y, df_m2_sam\n\t\t\n\tdf_m1_x_c1, df_m2_x_c1, df_m1_y_c1, df_m2_y_c1, df_m2_sam_c1 = f_featuring(df_m1_tt_c1, df_m2_tt_c1)\n\tdf_m1_x_c2, df_m2_x_c2, df_m1_y_c2, df_m2_y_c2, df_m2_sam_c2 = f_featuring(df_m1_tt_c2, df_m2_tt_c2)\n\n\t## Model Fitting\t\t\n\tgbl = globals()\n    # globals() 함수는 특정 모듈내에서 사용되는 전역 변수들의 변수 이름 및 현재 값 등을 알고 싶을 때 사용하는 함수. \n    # globals 함수는 코드내의 전역 변수의 이름과 현재 값을 출력. \n    # 여기서 전역변수란, 지역변수랑 반대되는 개념으로 하나의 반복문이나 조건문, 사용자정의함수 등에서 변수를 사용할 때 해당 변수를 구문 밖에서도 사용할 수 있는 변수를 의미.\n    \n    \n\tdef f_modeling_1(case_flg):\n\n\t\tdf_m1_x = gbl['df_m1_x_' + case_flg]\n\t\tdf_m2_x = gbl['df_m2_x_' + case_flg]\n\t\tdf_m1_y = gbl['df_m1_y_' + case_flg]\n\t\tdf_m2_y = gbl['df_m2_y_' + case_flg]\t\n\t\tdf_m2_sam = gbl['df_m2_sam_' + case_flg]\n\t\ttrain_data = lgb.Dataset(df_m1_x, label = df_m1_y)\t\n\t\tparameters = {\n\t\t\t'n_leaves':10,\n\t\t\t'n_estimators':80, \n\t\t\t'max_depth':-1,  \n\t\t\t'learning_rate':0.01, \n\t\t\t'subsample':1,\n\t\t\t'colsample_bytree':0.8,\n\t\t\t'reg_alpha':0.1,\n\t\t\t'reg_lambda':1,\n\t\t\t'objective': 'regression',\n\t\t\t'min_data':1,\n\t\t\t'min_data_in_bin':1,\t\n\t\t\t'random_state' : 123} \t\t\t\n        # 인자 설명은 앞선 2장에서 다루었습니다.\n        \n\t\tfit = lgb.train(parameters, train_data, num_boost_round=5000)\t\n\t\tdf_m1_pre = pd.DataFrame(fit.predict(df_m1_x))\n\t\tdf_m2_pre = pd.DataFrame(fit.predict(df_m2_x))\n\t\t\t\t\t\t\t\n\t\t## Predict\n\t\tdf_m2_pre.rename(columns = {0:\"score\"}, inplace = True)\n\t\tdf_m2_pre = pd.concat([df_m2_pre.reset_index(drop=True), \n\t\t\t\t\t\t\t   df_m2_sam[['Target', 'dt', 'code', 'amt', 'end']].reset_index(drop=True)], axis=1)\t\t\n\t\tdf_m2_pre['end_a1'] = df_m2_pre['end'] * df_m2_pre['Target']\t\t\t\n\t\tdf_m2_pre['end_a1_pred'] = df_m2_pre['end'] * df_m2_pre['score'] \n\t\tdf_m2_pre['model'] = 'M1' \n\t\tdf_m2_pre['case'] = case_flg\n\t\tdf_m2_pre['look_after'] = look_after\n\n\t\treturn df_m2_pre\n\n\tdf_m2_pre_c1 = f_modeling_1('c1')       \n\tdf_m2_pre_c2 = f_modeling_1('c2')\t\n\tdf_m2_pre_c1_tot = df_m2_pre_c1_tot.append(df_m2_pre_c1)\n\tdf_m2_pre_c2_tot = df_m2_pre_c2_tot.append(df_m2_pre_c2)",
      "metadata": {
        "trusted": true
      },
      "execution_count": 9,
      "outputs": [
        {
          "name": "stdout",
          "text": "\n ===> look_after :  1\n[LightGBM] [Warning] Unknown parameter: n_leaves\n[LightGBM] [Warning] Unknown parameter: n_leaves\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002100 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 29841\n[LightGBM] [Info] Number of data points in the train set: 800, number of used features: 120\n[LightGBM] [Info] Start training from score 1.000430\n[LightGBM] [Warning] Unknown parameter: n_leaves\n[LightGBM] [Warning] Unknown parameter: n_leaves\n[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000600 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 29841\n[LightGBM] [Info] Number of data points in the train set: 800, number of used features: 120\n[LightGBM] [Info] Start training from score 1.000730\n\n ===> look_after :  2\n[LightGBM] [Warning] Unknown parameter: n_leaves\n[LightGBM] [Warning] Unknown parameter: n_leaves\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001400 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 29841\n[LightGBM] [Info] Number of data points in the train set: 400, number of used features: 120\n[LightGBM] [Info] Start training from score 1.000858\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Unknown parameter: n_leaves\n[LightGBM] [Warning] Unknown parameter: n_leaves\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001100 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 29841\n[LightGBM] [Info] Number of data points in the train set: 400, number of used features: 120\n[LightGBM] [Info] Start training from score 1.000716\n\n ===> look_after :  3\n[LightGBM] [Warning] Unknown parameter: n_leaves\n[LightGBM] [Warning] Unknown parameter: n_leaves\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000500 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 23028\n[LightGBM] [Info] Number of data points in the train set: 200, number of used features: 120\n[LightGBM] [Info] Start training from score 1.002942\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Unknown parameter: n_leaves\n[LightGBM] [Warning] Unknown parameter: n_leaves\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000800 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 23227\n[LightGBM] [Info] Number of data points in the train set: 200, number of used features: 120\n[LightGBM] [Info] Start training from score 1.002424\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n ===> look_after :  4\n[LightGBM] [Warning] Unknown parameter: n_leaves\n[LightGBM] [Warning] Unknown parameter: n_leaves\n[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000300 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 11630\n[LightGBM] [Info] Number of data points in the train set: 100, number of used features: 120\n[LightGBM] [Info] Start training from score 1.000973\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Unknown parameter: n_leaves\n[LightGBM] [Warning] Unknown parameter: n_leaves\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000300 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11630\n[LightGBM] [Info] Number of data points in the train set: 100, number of used features: 120\n[LightGBM] [Info] Start training from score 1.003380\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n ===> look_after :  5\n[LightGBM] [Warning] Unknown parameter: n_leaves\n[LightGBM] [Warning] Unknown parameter: n_leaves\n[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000200 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 6053\n[LightGBM] [Info] Number of data points in the train set: 50, number of used features: 120\n[LightGBM] [Info] Start training from score 0.999910\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Unknown parameter: n_leaves\n[LightGBM] [Warning] Unknown parameter: n_leaves\n[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000000 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 5910\n[LightGBM] [Info] Number of data points in the train set: 50, number of used features: 120\n[LightGBM] [Info] Start training from score 1.001304\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "\n**여기서 잠깐!** ✋\n\n\n> [Warning] No further splits with positive gain, best gain: -inf \n\n\n이라는 경고문구가 많이 뜰 텐데요. 원래 위 코드는 모든 주식종목의 데이터를 활용하는 코드인데, 우리는 삼성전자의 데이터만 다루고 있기 때문에 데이터가 부족해서\n위와 같은 경고문구가 뜰 것입니다. 무시하고 넘어가셔도 좋아요 :)",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "이상으로 ‘모델 소개 및 학습’ 주제 Stage 5이였습니다.\n마지막 Stage인 Stage 6에는 최종적으로 예측된 데이터를 후처리 해주고, 제출 파일까지 만드는 내용으로 마무리 짓도록 하겠습니다.\n\n다음에 또 만나요!🫶",
      "metadata": {}
    }
  ]
}