{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "[코알라] 주식 종료 가격 예측 경진대회 Stage 3 : 데이터 전처리\n\n안녕하세요. 데이크루 4기 코알라🐨 팀입니다. \n\n저희는 📈주식 종료 가격 예측 경진대회를 주제로 PBL를 수행합니다.\n\n이번 활동을 통해 논리적인 접근 방식으로 모든 문제를 풀어갈 수 있는 데이커가 되는 것을 최종 목표로 하고 있습니다. \n\nStage 3은 PBL을 구성하는 6가지 단계 중 세 번째 단계로서 데이터 전처리 내용을 담고있습니다.\n\n다음의 포스팅은 데이크루 4기 활동으로 인하여 작성되었음을 알려드립니다.\n\n\n# Stage 3\n# 1. Review\n\n\n먼저 Stage 3에 들어가기 앞서, Stage 3를 원활하게 수행할 수 있도록 파일을 로드하고, 패키지를 로드하겠습니다. 이 과정에서 이전에서 학습했던 코드를 한번씩 복습해보도록 하겠습니다.\n\n\n각각의 파일이 어떠한 데이터인지 까먹으셨을까봐 다시 한 번 설명해드릴게요!\n\n\n- stock_list : 주식의 종목명과 종목코드, 상장 시장(Kospi 또는 Kosdaq)\n- sample_submission : 예측한 값을 기록하여 평가하기 위한 제출 파일의 양식\n\n- 종목리스트 : 패키지를 이용해 불러온 주식의 종목명과 종목코드, 상장시장 등 주식종목에 대한 여러 정보\n- 주가파일 : 2012년 1월 1일부터 2021년 11월 26일까지의 삼성전자 주가 데이터\n- 코스피상폐현황 : 2021년 11월 26일까지 코스피 시장에서 상장폐지된 종목\n- 코스닥상폐현황: 2021년 11월 26일까지 코스닥 시장에서 상장폐지된 종목\n- 주가지수 : 2012년 1월 1일부터 2022년 1월 18일까지 코스피/코스닥 지수\n- 환율파일 : 2012년 1월 1일부터 2022년 1월 18일까지 환율\n\n\npandas 패키지의 read_csv() 함수를 사용해 위 파일들을 한번 불러와봅시다. \n\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# 데이터를 불러오기 위해 pandas 패키지를 import\n\n# stock_list.csv 파일을 불러와서 stock_list 객체에 저장\n\n# 종목리스트.csv 파일을 불러와서 df_in_tt 객체에 저장\n\n# 주가파일.csv 파일을 불러와서 df_price 객체에 저장\n\n# 코스피상폐현황.csv 파일을 불러와서 df_out_tt1 객체에 저장\n\n# 코스닥상폐현황.csv 파일을 불러와서 df_out_tt2 객체에 저장\n\n# 주가지수.csv 파일을 불러와서 df_index 객체에 저장\n\n# 환율파일.csv 파일을 불러와서 df_usdkrw 객체에 저장  \n",
      "metadata": {},
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Inst.\n\n\npandas 패키지를 pd라는 약자로 import 하세요.\n\n\nread_csv() 메소드를 이용해 파일을 지정된 객체에 저장하세요\n\n\n이때, 데이터 파일은 현재 실행중인 파일(ipynb) 파일과 다른 위치에 있다고 생각하고 불러와보세요\n\n\n각각의 파일은 드라이브에 practice_data 폴더를 생성한 후 여기에 저장해주세요.\n\n\n파일에 한글이 들어있으면 read_csv() 함수 안에 encoding=’utf-8’을 지정해주어야 합니다!\n\n\n### Hint.\n```python\npd.read_csv('파일경로/파일명.확장자명', encoding='utf-8')\n```\n### Solution.\n\n\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import pandas as pd\nstock_list = pd.read_csv('./practice_data/stock_list.csv')\ndf_in_tt = pd.read_csv('./practice_data/종목리스트.csv', encoding='utf-8')\ndf_price = pd.read_csv('./practice_data/주가파일.csv', encoding='utf-8')\ndf_out_tt1 = pd.read_csv('./practice_data/코스피상폐현황.csv', encoding='utf-8')\ndf_out_tt2 = pd.read_csv('./practice_data/코스닥상폐현황.csv', encoding='utf-8')\ndf_index = pd.read_csv('./practice_data/주가지수.csv', encoding='utf-8')\ndf_usdkrw = pd.read_csv('./practice_data/환율파일.csv', encoding='utf-8')",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "데이터를 잘 불러오셨나요? \n\n\n문제 없이 잘 불러왔다면, 데이터 분석에 활용하기 위한 패키지를 로드해보겠습니다!\n\n\n자, 그러면 패키지를 설치하는 방법도 알았으니 앞서 말씀드린 패키지를 모두 불러와볼까요?\n\n\n위의 정답을 확인하지말고, 직접 없는 패키지는 설치해가며 따라해보세요!\n\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# pandas 패키지를 pd로 import\n\n# numpy 패키지를 np로 import\n\n# datetime 패키지를 import\n\n# requests 패키지를 req로 import\n\n# lightgbm 패키지를 lgb로 import\n\n# sklearn 패키지의 linear_model, metrics를 import\n\n# sklearn 패키지의 impute 안에 있는 SimpleImputer을 import\n\n# sklearn 패키지의 preprocessing 안에 있는 StandardScaler, RobustScaler, MinMaxScaler를 import\n\n# sklearn 패키지의 ensemble 안에 있는 RandomForestClassifier, RandomForestRegressor을 import\n\n# sklearn 패키지의 feature_selection에 있는 SelectKBest, f_regression, f_classifi을 import\n\n# os, sys, random, warnings 패키지를 import\n",
      "metadata": {},
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "\n## Hint.\n하나의 패키지 안에 들어있는 또 다른 모듈, 또는 하나의 패키지 안에 있는 모듈 속의 모듈을 로드할 때는 from 패키지.모듈 import 모듈 문을 사용합니다. \n\n\n패키지가 없다고 입력되면, 패키지를 설치하고 다시 실행해보세요!\n패키지를 설치하는 코드는 다음과 같습니다.\n\n\n```python\nimport micropip\nawait micropip.install('사용하려는 패키지')\n```\n\n\n(sklearn 패키지의 경우 설치를 할 때는 scikit-learn 이라는 패키지명으로 설치됩니다.)\n\n\n## Solution\n\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# pandas 패키지를 pd로 import\nimport pandas as pd\n# numpy 패키지를 np로 import\nimport numpy as np\n# datetime 패키지를 import\nimport datetime\n# requests 패키지를 req로 import\nimport requests as req\n# lightgbm 패키지를 lgb로 import\nimport lightgbm as lgb\n# sklearn 패키지의 linear_model, metrics를 import\nfrom sklearn import linear_model, metrics\n# sklearn 패키지의 impute 안에 있는 SimpleImputer을 import\nfrom sklearn.impute import SimpleImputer\n# sklearn 패키지의 preprocessing 안에 있는 StandardScaler, RobustScaler, MinMaxScaler를 import\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler \n# sklearn 패키지의 ensemble 안에 있는 RandomForestClassifier, RandomForestRegressor을 import\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n# sklearn 패키지의 feature_selection에 있는 SelectKBest, f_regression, f_classifi을 import\nfrom sklearn.feature_selection import SelectKBest, f_regression, f_classif\n# os, sys, random, warnings 패키지를 import\nimport os, sys, random, warnings",
      "metadata": {},
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "여기까지 문제없이 따라오셨다구요? 잘했습니다!\n\n\n<span style=\"color:red; font-size:120%\">그럼 이제 본격적으로 Stage 3, 데이터 전처리를 시작하겠습니다!</sapn>\n\n\n\n# 2. 데이터 전처리\n\n\n데이터를 불러왔으면, 분석하려는 문제에 맞추어 데이터를 가공해야합니다!\n\n\n여러분은 혹시 **“Garbage In, Garbage Out”**이라는 말을 들어보셨나요?\n\n\n해석하면 **“쓰레기가 들어가면 쓰레기가 나온다”** 라는 뜻으로, 말 그대로 쓰레기(잘 가공되지 않은 데이터)가 들어가면 쓰레기(이상한 분석결과)가 나온다는 것을 의미합니다.\n\n\n<img src = \"https://i0.wp.com/www.dodomira.com/wp-content/uploads/2016/10/Time-1200x511.jpg?w=960 \" height = 400 width = 800 >\n\n\n데이터 과학자는 데이터 분석을 할 때 60% 정도의 시간을 데이터를 정리하고 구성하는데 사용하며, 데이터를 수집하는 시간은 19% 정도입니다.\n\n\n이는 데이터 과학자가 분석을 위해 데이터를 준비하고 관리하는데만 약 80%의 시간을 소비한다는 것을 의미하는데요.\n\n\n이처럼 데이터 전처리는 데이터 분석 과정 속에서 가장 많은 시간이 투입되며, 어떻게 보면 가장 중요한 과정이라고 할 수 있습니다.\n\n\n\n데이터 전처리는 데이터에서 필요없는 변수를 제거하고, 중복값을 처리하고, 결측값을 대체하며, 새로운 변수를 생성하는 등 다양한 일을 수행합니다.\n\n\n오늘 우리는 이 중에서 다음과 같은 처리 과정을 거칠 것입니다.\n\n\n- #### 데이터프레임의 컬럼명 변경\n\n\n- #### 변수의 데이터 유형 변경\n\n\n- #### 조건에 맞는 데이터 선택\n\n\n- #### 불필요한 변수 제거\n\n\n- #### 데이터프레임 합치기(concat)\n\n\n- #### 데이터프레임 병합(merge)\n\n\n\n우선, 불러온 데이터 중 날짜 데이터를 변환하겠습니다.",
      "metadata": {
        "tags": []
      }
    },
    {
      "cell_type": "code",
      "source": "df_price['dt'] = pd.to_datetime(df_price['dt'])\ndf_usdkrw['dt'] = pd.to_datetime(df_usdkrw['dt'])\ndf_index['dt'] = pd.to_datetime(df_index['dt'])",
      "metadata": {},
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "다음, 주식 종목코드를 정제하기 위한 코드를 작성해보겠습니다.\n\n\npandas 패키지를 자유자제로 다루며 전처리하는 과정이 익숙치 않으실테니, 코드별로 설명과 함께 학습한 후, 같은 코드를 정답을 보지 않고 작성할 수 있을 때까지 공부해보세요!\n\n<span style=\"background-color:plum; font-size:110%\"> **Market 변수를 새로 추가, df_out_tt1은 코스피상폐종목이므로 코스피로 값 입력**",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df_out_tt1['market'] = 'KOSPI' ",
      "metadata": {},
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "<span style=\"background-color:plum; font-size:110%\"> **Market 변수를 새로 추가, df_out_tt2은 코스닥상폐종목이므로 코스닥으로 값 입력**",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df_out_tt2['market'] = 'KOSDAQ'",
      "metadata": {},
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "데이터프레임의 변수(컬럼)을 다룰 때는, df[‘컬럼명’]의 방식으로 다룰 수 있습니다.\n\n\n기존에 있는 컬럼을 불러올 수도 있고, 위와 같이 새로운 컬럼을 생성할 수도 있죠.\n\n\ndf_out_tt1에 ‘Market’ 변수가 없는 상태에서 위와 같이 코드를 입력한다면, df_out_tt1에 ‘Market’ 변수가 새로 생기게 되고, 모든 값은 KOSPI로 가지게 될 것입니다.\n\n<span style=\"background-color:plum; font-size:110%\"> **두 데이터를 통합(concat)**\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df_out_tt = pd.concat([df_out_tt1, df_out_tt2])",
      "metadata": {},
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "두 데이터를 합칠 때는 pd.concat 함수와 pd.merge 함수가 있는데요,\npd.merge 함수는 두 데이터가 공통되게 가지고 있는 컬럼을 기준으로 데이터를 합치는 명령어이고, pd.concat 함수는 두 데이터를 행 또는 열로 추가하는 명령어입니다.\n\n다음과 같은 데이터 프레임이 있다고 가정해봅시다.\n\n- ### df1\n\n<img src = \"https://cherry-orbit-69a.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fb9e9638d-55f1-4e49-b89c-21920e198480%2F%25ED%2599%2594%25EB%25A9%25B4_%25EC%25BA%25A1%25EC%25B2%2598_2023-02-22_000456.png?id=24fd9953-7e80-4f68-9086-f7465cadb337&table=block&spaceId=9b3f14ce-6c75-4c95-8989-493ed6e93a8a&width=130&userId=&cache=v2\">\n\n- ### df2\n\n<img src = \"https://cherry-orbit-69a.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F90066136-f39f-4ac7-ae92-288b02d74931%2F%25ED%2599%2594%25EB%25A9%25B4_%25EC%25BA%25A1%25EC%25B2%2598_2023-02-22_000524.png?id=07d3134a-76f6-4c72-b6d5-ebff252e7f53&table=block&spaceId=9b3f14ce-6c75-4c95-8989-493ed6e93a8a&width=170&userId=&cache=v2\">\n\n\n\n만약 **pd.merge( )**를 사용해 두 데이터를 병합한다면, 병합된 데이터는 다음과 같을 것입니다.\n\n<img src = \"https://cherry-orbit-69a.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F4f954ea8-99ad-458f-abd5-ca49d75b44a4%2F%25ED%2599%2594%25EB%25A9%25B4_%25EC%25BA%25A1%25EC%25B2%2598_2023-02-22_000541.png?id=65061d39-7449-4cde-8ef4-7a9f08824c7f&table=block&spaceId=9b3f14ce-6c75-4c95-8989-493ed6e93a8a&width=250&userId=&cache=v2\">\n\n\n**pd.concat**을 사용해 두 데이터를 열으로 병합한다면, 데이터는 다음과 같습니다.\n\n\n<img src = \"https://cherry-orbit-69a.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F2d197737-eabc-4436-a424-b49ab63415e4%2F%25ED%2599%2594%25EB%25A9%25B4_%25EC%25BA%25A1%25EC%25B2%2598_2023-02-22_000604.png?id=51913d47-11c9-4234-af16-93293e887c8a&table=block&spaceId=9b3f14ce-6c75-4c95-8989-493ed6e93a8a&width=300&userId=&cache=v2\">\n\n**pd.concat**을 사용해 두 데이터를 행으로 병합한다면, 데이터는 다음과 같습니다.\n\n<img src = \"https://cherry-orbit-69a.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F4ca3c3b0-050f-40d1-9914-82e3c02e23fb%2F%25ED%2599%2594%25EB%25A9%25B4_%25EC%25BA%25A1%25EC%25B2%2598_2023-02-22_000621.png?id=61c7b2e5-0140-4479-83e1-2c9c556d65e3&table=block&spaceId=9b3f14ce-6c75-4c95-8989-493ed6e93a8a&width=250&userId=&cache=v2\">\n\n\n\n\n이제 차이를 아시겠나요?\npd.concat 함수에서 행과 열을 구분하는 인자는 axis인데, **axis=1로 지정하면 열로 추가되고, axis=0으로 지정하거나 지정하지 않으면 행으로 추가됩니다.**\n\n\n\n\n<span style=\"background-color:plum; font-size:110%\"> **컬럼명 변경**\n\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df_out_tt = df_out_tt.rename(columns = {\"종목코드\":\"code\", \"회사명\":\"code_nm\", \"폐지일자\":\"out_dt\", \"폐지사유\":\"out_desc\"})",
      "metadata": {},
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "\n\n데이터의 컬럼명을 변경하는 방법은 두가지가 있습니다.\n\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "```python\ndf.rename(columns={'기존컬럼1':'바꿀컬럼1', '기존컬럼2':'바꿀컬럼2', … })\ndf.columns = ['바꿀컬럼1', '바꿀컬럼2', '바꿀컬럼3', …]\n```",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "\n\nrename 함수의 장점은 바꾸고 싶은 컬럼명만 지정해서 바꿀 수 있다는 것입니다.\n\n\n2번째 방법인 columns 함수를 쓴다면, 바꾸고 싶지 않은 컬럼이 있어도 컬럼의 위치에 맞게 컬럼명을 모두 지정해주어야 합니다.\n\n여기서는 바꿀 컬럼명이 많지 않기 때문에, rename 함수로 직접 지정하였습니다.\n\n\n<span style=\"background-color:plum; font-size:110%\">**데이터 유형 변경**\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df_out_tt['code'] = df_out_tt['code'].astype(str) ",
      "metadata": {},
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "<span style=\"background-color:plum; font-size:110%\">**out_dt 변수의 데이터 유형을 날짜 형식으로 변경**\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df_out_tt['out_dt'] = pd.to_datetime(df_out_tt['out_dt'])",
      "metadata": {},
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "다음은 데이터 유형을 변경하는 코드입니다.\n대부분의 데이터는 df[‘컬럼명’].astype(데이터유형)을 통해 바꿀 수 있는데, pd.to_datetime으로도 바꿀 수 있습니다.\n\n\nastype 안에 들어갈 수 있는 데이터 유형 인자는 str(문자), int(실수), float(정수) 등이 대표적입니다.\n\n<span style=\"background-color:plum; font-size:110%\">**code 변수를 6자리 숫자로 만들기**",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df_out_tt['code'] = df_out_tt['code'].str.zfill(6) ",
      "metadata": {},
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "\n.zfill() 메서드는 괄호 안에 있는 수에 맞게 적합한 자릿수를 맞춥니다. 부족한 부분은 0으로 채웁니다.\n\n예를 들어, 기존에 123456이던 데이터를 123456으로, 1234이던 데이터는 001234로 변환합니다.\n\n<span style=\"background-color:plum; font-size:110%\">**df_price 데이터의 code를 6자리 숫자로 만들기**",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df_price['code'] = df_price['code'].astype(str)\ndf_price['code'] = df_price['code'].str.zfill(6) ",
      "metadata": {},
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "마찬가지로, df_price의 code 컬럼은 숫자로 구성되어 있습니다.\n\n우선, 이 데이터의 유형을 str(string, 문자열)로 변경한 뒤, 위와 같은 원리로 6자리 숫자로 채워줍니다.\n\n\n<span style=\"background-color:plum; font-size:110%\">**번호, 비고 컬럼을 제거**",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df_out_tt = df_out_tt.drop(['번호', '비고'], axis=1)",
      "metadata": {},
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "drop 메서드는 리스트로 저장된 열 이름 [‘번호’, ‘비고’]를 데이터프레임에서 제거합니다. 이때 axis = 1은 열 방향으로 drop 메서드를 실행시킨다는 뜻을 의미합니다.\n\n<span style=\"background-color:plum; font-size:110%\">**변수명 변경**\n\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df_in_tt.columns = ['code', 'isu_cd', 'code_nm', 'market', 'dept', 'close', 'changecode', \n                    'changes', 'chagesratio', 'open', 'high', 'low', 'volume', 'amount', 'marcap', 'stocks', 'MarketId']",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "위 코드를 **rename** 방식으로 바꾸면 어떨까요?",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "```python\ndf_in_tt.rename(columns = {'Code': 'code', 'ISU_CD': 'isu_cd', 'Name': 'code_nm','Market': 'market', 'Dept' : 'dept', \n                           'Close': 'close', 'ChangeCode': 'changecode', 'Changes': 'changes', 'ChagesRatio': 'chagesratio',\n                           'Open': 'open','High': 'high', 'Low': 'low', 'Volume': 'volume', 'Amount': 'amount',\n                           'Marcap':'marcap','Stocks': 'stocks','MarketId': 'marketId'})\n```",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "rename 메서드를 사용하면 위 columns 메서드와 동일한 결과가 출력됩니다\n\n\n<span style=\"background-color:plum; font-size:110%\">**조건에 맞는 데이터 선택**\n\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df_in_tt = df_in_tt[(df_in_tt['market'] !='KONEX') &                \n                    (df_in_tt.code.str.startswith('5') != True) &   \n                    (df_in_tt.code.str.startswith('6') != True) &   \n                    (df_in_tt.code.str.startswith('7') != True) &   \n                    (df_in_tt.code.str.len() == 6)]",
      "metadata": {},
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "상장되어 있는 기업 명단을 앞서 정한 조건에 맞게 뽑아내기 위해서, 상장기업명단을 조건문을 이용하여 다시 선언하였습니다. 얼핏 보면 살짝 어려울 수 있는데, 자세히 알아봅시다!\n\n위에서 필요한 열만 추출한 데이터 프레임을 조건에 맞는 행만 선택 할 수 있는 코드입니다.\n\n추출하기 위한 조건이 2개 이상인 경우 각 조건을 괄호로 묶어주는 것이 안전하며 조건1과 조건2가 있을 때, **이 두 조건을 모두 만족하는 데이터를 추출하고 싶다면 &을 사용해주고 두 조건 중 하나라도 조건을 만족하는 데이터를 추출하고 싶다면 | 를 사용해주면 됩니다.**\n\n\n```python\n# 조건을 둘다 만족해야 할때\ndf[(조건1) & (조건2)]\n\n# 조건 중 하나라도 만족하면 될때\ndf[(조건1) | (조건2)]\n```\n\n---\n\ndf_in_tt[(df_in_tt['market'] !='KONEX') &  \n- [X] (df_in_tt 내 열 이름 market에 해당하는 값이 ‘ KONEX’가 아닌 행) 그리고\n\n(df_in_tt.code.str.startswith('5') != True) &  (df_in_tt.code.str.startswith('6') != True) & (df_in_tt.code.str.startswith('7') != True) &   \n- [X] (df_in_tt 내 code의 값이 5, 6, 7로 시작하지 않는 행) \n\n그리고 이 때, df_in_tt.code는 df_in_tt['code']와 같은 의미입니다. \n\n (df_in_tt.code.str.len() == 6)]\n- [X] df_in_tt 내 code의 문자열의 길이가 6인 행을 추출한다\n\n라는 의미를 가지고 있습니다. \n\n따라서, 새로 정의된 상장기업명단 df_in_tt는\ndf_in_tt의 market 값이 ‘KONEX’가 아니며, code의 첫글자가 5,6,7로 시작하지 않고, code의 문자열의 총 길이가 6인 행을 추출한 데이터프레임이라고 할 수 있습니다.  \n                 \n<span style=\"background-color:plum; font-size:110%\">**변수 추가**\n\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "새로 정의된 df_in_tt에 ‘in_yn’이라는 변수명으로 모든 값이 1인 열을 추가합니다. \n\n<span style=\"background-color:plum; font-size:110%\">**오늘 날짜 입력**",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "end_dt = datetime.datetime.now().strftime('%Y%m%d')   ",
      "metadata": {},
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "\n**datetime 모듈**을 이용하여 오늘 날짜를 기입하고, 형식은 ‘년-월-일’ 순으로 나열하도록 바꿉니다. 형식을 지정해주지 않으면 디폴트 값으로 ‘년-월-일-시-분-초’까지 보여지게 됩니다.\n\n<span style=\"background-color:plum; font-size:110%\">**데이터 병합(merge)**\n\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df_stock_code = pd.merge(df_out_tt, df_in_tt, how='outer', on='code', suffixes=('_old', '_new'))",
      "metadata": {},
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "\n\n앞서 배운 **merge 메서드로 두 데이터프레임 df_out_tt와 df_in_tt를 병합**합니다. \n\n이 때 두 데이터프레임에서 code를 기준으로 병합하되 둘 중 하나라도 값을 가지고 있지 않으면\n\n결측값인 NaN 값을 할당하게끔 하며, suffixes는 df_out_tt의 열에 ‘_old’ 접사를, df_in_tt 열에‘_new’ 접사를 붙이게 합니다. \n\n실제로 실행하면 데이터프레임 내 상당한 NaN 값이 존재하는 것을 볼 수 있습니다.\n\n\n<span style=\"background-color:plum; font-size:110%\">**변수 값 추가(또는 수정)**\n\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df_stock_code['in_yn'] = np.where(df_stock_code['in_yn'].notnull(), df_stock_code['in_yn'], 0)",
      "metadata": {},
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "**numpy 모듈의 where 메서드는 조건문**이라고 할 수 있습니다. 위 코드를 풀어보겠습니다. \n\ndf_stock_code의 in_yn 열의 값이 notnull()의 조건을 만족하지 못하면, 즉 df_stock_code의 in_yn값이 결측값(=NaN)이라면 그 값을 0으로 대체한다는 의미를 가지고 있습니다.\n\n\n<span style=\"background-color:plum; font-size:110%\"> **market_new가 결측치가 아니면 market_new를 market 값으로, 결측치면 market_old를 market 값으로**\n\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df_stock_code['market'] = np.where(df_stock_code['market_new'].notnull(), df_stock_code['market_new'], df_stock_code['market_old'])\n\n# 위와 같은 원리\ndf_stock_code['code_nm'] = np.where(df_stock_code['code_nm_new'].notnull(), df_stock_code['code_nm_new'], df_stock_code['code_nm_old'])",
      "metadata": {},
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "<span style=\"background-color:plum; font-size:110%\">**end_dt 값으로 base_dt 컬럼을 추가**\n\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df_stock_code['base_dt'] = pd.to_datetime(end_dt)",
      "metadata": {},
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "<span style=\"background-color:plum; font-size:110%\">**컬럼명 변경**\n\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df_stock_code = df_stock_code.rename(columns = {\"ListingDate\":\"in_dt\"}) ",
      "metadata": {},
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "\n\n**rename** 메서드에서는 바꾸고 싶은 칼럼명만 선언하면 되지만, 앞선 columns 속성을 활용한 방식은 바꾸고 싶은 칼럼명 뿐만 아니라 모든 칼럼명을 나열해야 합니다. \n\n이 경우 효율성 측면에서  rename 방식이 더 적합합니다.\n\n<span style=\"background-color:plum; font-size:110%\">**데이터 정렬**\n\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df_stock_code = df_stock_code.sort_values(by=['code', 'base_dt'], axis=0, ascending=[True, False])  ",
      "metadata": {},
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "**sort_values** 메서드는 데이터프레임 내 데이터를 정렬합니다. \n\nby는 기준을 제시하는 데, 리스트로 주어지면 왼쪽부터 순서대로 기준을 잡는다고 생각할 수 있습니다. \n\n이 경우 code를 기준으로 먼저 정렬하고, 같은 값이 존재할 경우 base_dt를 기준으로 정렬한다는 것을 알 수 있습니다. axis = 0 이므로 행방향으로 정렬임을 알 수 있습니다. \n\nascending, 오름차순으로 정렬할 것인지에 대한 설정이 [True, False] 이므로 code를 기준으로는 오름차순으로, base_dt를 기준으로 잡았을 때는 내림차순으로 정렬합니다.\n\n\n<span style=\"background-color:plum; font-size:110%\">**중복값 제거**\n\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df_stock_code = df_stock_code.drop_duplicates(['code'], keep='first')",
      "metadata": {},
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "**drop_duplicates()** 메서드는 중복값을 제거합니다. argument keep=’first’는 첫번째로 등장한 값만 유지하고 나머지는 제거하는 것을 알 수 있습니다. \n\ncode 값이 동일한 행들을 정리하는 부분이라고 할 수 있습니다.\n\n<span style=\"background-color:plum; font-size:110%\">**필요한 변수만 선택(불필요한 변수 제거)**\n\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df_stock_code = df_stock_code[['code','code_nm','in_yn','market','out_dt','out_desc','base_dt']]",
      "metadata": {},
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "\n\n파이썬에서는 칼럼명을 이용해서 필요한 열을 추출할 수 있습니다.\n\n```python\nDataframe[['칼럼명1', '칼럼명2', ∙∙∙]]\n```\n\n즉, 추출하고자 하는 칼럼명들을 리스트에 담아 데이터프레임 인덱싱으로 넘겨주면 됩니다. 이렇게 하면 어떤 칼럼을 추출했는지 코드상 가장 이해하기 쉬워 가장 많이 사용하는 방법입니다.\n\n\n위 코드는 데이터프레임의 ‘code’, ‘code_nm’, ‘in_yn’, ‘market’, ‘out_dt’, ‘out_desc’, ‘base_dt’ 열만 추출하여 df_stock_code 데이터프레임에 덮어 씌워주는 코드입니다.\n\n\n<span style=\"background-color:plum; font-size:110%\">**조건에 맞는 데이터 선택**\n\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df_stock_code = df_stock_code[df_stock_code['code']=='005930']",
      "metadata": {},
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "위 코드는 df_stock_code에서 종목코드가 '005930'인, 즉 삼성전자 데이터만 가져오는 코드입니다.\n\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "\n이상으로 ‘데이터 전처리’ 주제 Stage 3이였습니다.\n\n다음 Stage 4에는 전처리된 데이터를 바탕으로 탐색적 데이터 분석(EDA)을 진행하고, 필요한 변수를 추가하는 내용을 가지고 찾아오도록 하겠습니다.\n\n<span style=\"color:red; font-size:120%\">다음에 또 만나요!\n\n",
      "metadata": {}
    }
  ]
}